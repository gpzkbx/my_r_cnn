{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3 モデルのcheckpointとVOCデータの取得を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "TF_MODELS_URL = \"http://download.tensorflow.org/models\"\n",
    "INCEPTION_V3_URL = TF_MODELS_URL + \"/inception_v3_2016_08_28.tar.gz\"\n",
    "INCEPTION_PATH = os.path.join(\"datasets\", \"inception\")\n",
    "INCEPTION_V3_CHECKPOINT_PATH = os.path.join(INCEPTION_PATH, \"inception_v3.ckpt\")\n",
    "\n",
    "def download_progress(count, block_size, total_size):\n",
    "    percent = count * block_size * 100 // total_size\n",
    "    sys.stdout.write(\"\\rDownloading: {}%\".format(percent))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def fetch_pretrained_inception_v3(url=INCEPTION_V3_URL, path=INCEPTION_PATH):\n",
    "    if os.path.exists(INCEPTION_V3_CHECKPOINT_PATH):\n",
    "        return\n",
    "    os.makedirs(path)\n",
    "    tgz_path = os.path.join(path, \"inception_v3.tgz\")\n",
    "    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
    "    inception_tgz = tarfile.open(tgz_path)\n",
    "    inception_tgz.extractall(path=path)\n",
    "    inception_tgz.close()\n",
    "    os.remove(tgz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_URL = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
    "VOC_DOWNLOAD_PATH = os.path.join('datasets', 'VOC')\n",
    "\n",
    "def fetch_voc(url = VOC_URL, path = VOC_DOWNLOAD_PATH):\n",
    "    if os.path.exists(VOC_DOWNLOAD_PATH):\n",
    "        print()\n",
    "        return\n",
    "    os.makedirs(path)\n",
    "    tgz_path = os.path.join(path, 'VOC_photos.tar')\n",
    "    urllib.request.urlretrieve(url, tgz_path, reporthook = download_progress)\n",
    "    voc_tgz = tarfile.open(tgz_path)\n",
    "    voc_tgz.extractall(path = path)\n",
    "    voc_tgz.close()\n",
    "    os.remove(tgz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_pretrained_inception_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_voc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像のパスを全部取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_DATA_HOME = os.path.join(VOC_DOWNLOAD_PATH, 'VOCdevkit', 'VOC2012')\n",
    "\n",
    "image_paths = list()\n",
    "annotation_paths = list()\n",
    "\n",
    "image_dir = os.path.join(VOC_DATA_HOME, 'JPEGImages')\n",
    "for filepath in os.listdir(image_dir):\n",
    "    if filepath.endswith('jpg'):\n",
    "        image_paths.append(os.path.join(image_dir, filepath))\n",
    "    \n",
    "annotation_dir = os.path.join(VOC_DATA_HOME, 'Annotations')\n",
    "for filepath in os.listdir(annotation_dir):\n",
    "    if filepath.endswith('xml'):\n",
    "        annotation_paths.append(os.path.join(annotation_dir, filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17125\n",
      "JPEGImages/2007_000027.jpg\n",
      "JPEGImages/2007_004281.jpg\n",
      "17125\n",
      "Annotations/2007_000027.xml\n",
      "Annotations/2007_004281.xml\n"
     ]
    }
   ],
   "source": [
    "image_paths.sort()\n",
    "print(len(image_paths))\n",
    "print(image_paths[0])\n",
    "print(image_paths[301])\n",
    "\n",
    "annotation_paths.sort()\n",
    "print(len(annotation_paths))\n",
    "print(annotation_paths[0])\n",
    "print(annotation_paths[301])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotationの.xmlファイルに整数ではないbounding box の値が存在するので、そのようなファイルを探す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imresize\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_with_nonint_bndbox(annotation_paths):\n",
    "    wrong_xml_paths = []\n",
    "    for annotation_path in annotation_paths:\n",
    "        tree = ET.parse(annotation_path)\n",
    "        try:\n",
    "            bnd_box_roots = tree.findall('object/bndbox')\n",
    "            bnd_boxs = [(bnd_box_root.findtext('xmin'), bnd_box_root.findtext('ymin'),\n",
    "            bnd_box_root.findtext('xmax'), bnd_box_root.findtext('ymax'),) \n",
    "            for bnd_box_root in list(bnd_box_roots)]\n",
    "            \n",
    "            for i in range(len(bnd_boxs)):\n",
    "                a_ = int(bnd_boxs[i][0])\n",
    "                b_ = int(bnd_boxs[i][1])\n",
    "                c_ = int(bnd_boxs[i][2])\n",
    "                d_ = int(bnd_boxs[i][3])\n",
    "        except ValueError:\n",
    "            wrong_xml_paths.append(annotation_path)\n",
    "    return wrong_xml_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_xml_paths = get_list_with_nonint_bndbox(annotation_paths = annotation_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Annotations/2011_006777.xml']\n"
     ]
    }
   ],
   "source": [
    "print(wrong_xml_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像からRegion proposal部分の画素とラベルと抽出し、InceptionV3の入力次元にresizeする関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_proposal_and_label(image_path, annotation_path, target_width = 299, target_height = 299):\n",
    "    \"\"\"return a dict of region_proposals \n",
    "         the key of which is the object label and \n",
    "         the value is a list of the img and the bounding box\n",
    "         exp: {'person': [(fig1, bnd_box1), (fig2, bnd_box2)]}\"\"\"\n",
    "    \n",
    "    tree = ET.parse(annotation_path)\n",
    "    object_names = [object_name.text for object_name in tree.findall('object/name')]\n",
    "    bnd_box_roots = tree.findall('object/bndbox')\n",
    "    bnd_boxs = [(bnd_box_root.findtext('xmin'), bnd_box_root.findtext('ymin'),\n",
    "            bnd_box_root.findtext('xmax'), bnd_box_root.findtext('ymax'),) \n",
    "            for bnd_box_root in list(bnd_box_roots)]\n",
    "    \n",
    "    region_proposals = defaultdict(list)\n",
    "    \n",
    "    img = mpimg.imread(image_path)\n",
    "    for i in range(len(bnd_boxs)):\n",
    "        region_proposal = img[int(bnd_boxs[i][1]) : int(bnd_boxs[i][3]), int(bnd_boxs[i][0]) : int(bnd_boxs[i][2])]\n",
    "        region_proposal = imresize(region_proposal, (target_width, target_height))\n",
    "        region_proposal = region_proposal.astype(np.float32) / 255\n",
    "        region_proposals[object_names[i]].append((region_proposal, bnd_boxs[i]))\n",
    "    \n",
    "    return region_proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "('174', '101', '349', '351')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ai_course/lib/python2.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "region_proposals = get_region_proposal_and_label(image_path = os.path.join('JPEGImages', '2007_000027.jpg'),\n",
    "                                                annotation_path = os.path.join('Annotations/2007_000027.xml'))\n",
    "print(type(region_proposals[region_proposals.keys()[0]]))\n",
    "print(region_proposals[region_proposals.keys()[0]][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposal の画像データを新しいディレクトリーに保存し、画像のパスとラベルのリストを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_class_ids = {'person': 0, 'aeroplane': 1, 'bicycle': 2, 'bird': 3, 'boat': 4, 'bottle': 5, 'bus': 6, 'car': 7, 'cat': 8, \n",
    "                'chair': 9, 'cow': 10, 'diningtable': 11, 'dog': 12, 'horse': 13, 'motorbike': 14, 'pottedplant': 15, \n",
    "                'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle as pickle\n",
    "\n",
    "RP_IMAGES_PATH = os.path.join('rpimages')\n",
    "\n",
    "def save_region_proposal_images(image_paths = image_paths, annotation_paths = annotation_paths):\n",
    "    for image_path, annotation_path in zip(image_paths, annotation_paths):\n",
    "        region_proposals = get_region_proposal_and_label(image_path = image_path, \n",
    "                                                         annotation_path = annotation_path)\n",
    "        for label, imgs_with_bndbox in region_proposals.items():\n",
    "            for img_with_bndbox in imgs_with_bndbox:\n",
    "                now = datetime.datetime.now()\n",
    "                filename = '{0:%H%M%f}'.format(now)\n",
    "                np.save(os.path.join(RP_IMAGES_PATH, filename), img_with_bndbox[0])      #3/5ぐらいで39.8G消費した \n",
    "                #with open(os.path.join(RP_IMAGES_PATH, filename), 'wb') as f:      #毎回fileを開く動作が重い\n",
    "                    #pickle.dump(img_with_bndbox[0].tolist(), f)\n",
    "                #print(image_path)\n",
    "                \n",
    "                yield (os.path.join(RP_IMAGES_PATH, filename + '.npy'), voc_class_ids[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RP_IMAGES_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a211abccbb1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvoc_paths_and_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRP_IMAGES_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRP_IMAGES_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RP_IMAGES_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "voc_paths_and_classes = []\n",
    "\n",
    "if not os.path.exists(RP_IMAGES_PATH):\n",
    "    os.makedirs(RP_IMAGES_PATH)\n",
    "    \n",
    "voc_save_counter = 0\n",
    "    \n",
    "for mid_yield in save_region_proposal_images():\n",
    "    voc_paths_and_classes.append(mid_yield)\n",
    "    voc_save_counter += 1\n",
    "    if voc_save_counter >= 100:\n",
    "        voc_save_counter = 0\n",
    "        with open('voc_paths_and_classes', 'wb') as f:\n",
    "            pickle.dump(voc_paths_and_classes, f)\n",
    "            \n",
    "with open('voc_paths_and_classes', 'wb') as f:\n",
    "            pickle.dump(voc_paths_and_classes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a8e1bc228d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_paths_and_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_paths_and_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_paths_and_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshodw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(voc_paths_and_classes))\n",
    "example = np.load(os.path.join(voc_paths_and_classes)[40000][0])\n",
    "print(voc_paths_and_classes[40000])\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception v3モデルを読み込み、その出力層を変える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ai_course/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.nets import inception\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 299, 299, 3], name = 'X')\n",
    "traning = tf.placeholder_with_default(False, shape = [])\n",
    "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "    logits, end_points = inception.inception_v3(X, num_classes = 1001, is_training = traning)\n",
    "\n",
    "inception_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'InceptionV3/Logits/Dropout_1b/cond/Merge:0' shape=(?, 1, 1, 2048) dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_points['PreLogits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prelogits = tf.squeeze(end_points['PreLogits'], axis = [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 20\n",
    "\n",
    "with tf.name_scope('new_output_layer'):\n",
    "    voc_logits = tf.layers.dense(prelogits, n_outputs, name = 'voc_logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "y = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = voc_logits, labels = y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = 0.9, use_nesterov = True)\n",
    "    voc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'voc_logits')\n",
    "    training_op = optimizer.minimize(loss, var_list = voc_vars)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(voc_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "with tf.name_scope('init_and_save'):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'voc_logits/kernel:0', u'voc_logits/bias:0']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in voc_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データとテストデータの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def prepare_batch(voc_paths_and_classes, batch_size):\n",
    "    batch_paths_and_classes = sample(voc_paths_and_classes, batch_size)\n",
    "    images = [np.load(path) for path, labels in batch_paths_and_classes]\n",
    "    X_batch = 2 * np.stack(images) - 1 #inception expects colors ranging from -1 to 1\n",
    "    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype = np.int32)\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_paths_and_classes_train = voc_paths_and_classes[:30000]\n",
    "voc_paths_and_classes_validation = voc_paths_and_classes[30000:35000]\n",
    "voc_paths_and_classes_test = voc_paths_and_classes[35000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "5000\n",
      "5138\n"
     ]
    }
   ],
   "source": [
    "print(len(voc_paths_and_classes_train))\n",
    "print(len(voc_paths_and_classes_validation))\n",
    "print(len(voc_paths_and_classes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = prepare_batch(voc_paths_and_classes_train, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 299, 299, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation, y_validation = prepare_batch(voc_paths_and_classes_validation, batch_size = len(voc_paths_and_classes_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 299, 299, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 早期打ち切りを実装した学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params():\n",
    "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
    "\n",
    "def restore_model_params(model_params):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from datasets/inception/inception_v3.ckpt\n",
      "Epoch 0."
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 100\n",
    "n_iterations_per_epoch = len(voc_paths_and_classes_train) // batch_size\n",
    "\n",
    "best_loss_val = np.infty\n",
    "check_interval = 140\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 20\n",
    "best_model_params = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    inception_saver.restore(sess, INCEPTION_V3_CHECKPOINT_PATH)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch\", epoch, end = '')\n",
    "        for iteration in range(n_iterations_per_epoch):\n",
    "            if iteration % 3 == 0:\n",
    "                print('.', end = '')\n",
    "            X_batch, y_batch = prepare_batch(voc_paths_and_classes_train, batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch, traning: True})\n",
    "            if iteration % check_interval == 0:\n",
    "                loss_val = loss.eval(feed_dict = {X: X_validation, y: y_validation})\n",
    "                if loss_val < best_loss_val:\n",
    "                    best_loss_val = loss_val\n",
    "                    checks_since_last_progress = 0\n",
    "                    best_model_params = get_model_params()\n",
    "                else:\n",
    "                    checks_since_last_progress += 1\n",
    "        acc_train = accuracy.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict = {X: X_validation, y: y_validation})\n",
    "        print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
    "                  epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!!!\")\n",
    "            break\n",
    "    if best_model_params:\n",
    "        restore_model_params(best_model_params)\n",
    "    save_path = saver.save(sess, \"./voc_regionproposal_cnn\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
